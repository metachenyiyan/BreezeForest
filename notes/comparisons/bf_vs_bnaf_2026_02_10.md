# Method Comparison: BreezeForest vs BNAF (Block Neural Autoregressive Flow)
**Date**: 2026-02-10

## Overview
BreezeForest and BNAF are both autoregressive normalizing flows that act as universal density approximators. BNAF uses a single feed-forward network with block-triangular masked weight matrices and tracks Jacobian log-determinants analytically through the network layers (O(N^3) cost due to full block matrix operations). BreezeForest decomposes the problem into per-dimension "trees" connected by "breeze" bias links, computing the Jacobian via numerical finite differences (O(N^2) cost from lower-triangular diagonal structure). Both achieve universality, but BreezeForest trades BNAF's analytical elegance for a simpler, more modular architecture with cheaper Jacobian computation.

## Comparison Table

| Aspect | BreezeForest | BNAF |
|---|---|---|
| **Architecture** | | |
| Flow type | Autoregressive | Autoregressive |
| Building block | Tree layers + breeze connections | MaskedWeight layers (block-triangular) |
| Coupling mechanism | Conditional CDF via breeze bias | Block-triangular affine transforms |
| Depth requirement | Few layers (universal with 1 hidden layer) | Single feed-forward network (universal) |
| Parameterization | Softplus/squared weights (monotonicity) | exp(weight) on diagonal, free off-diagonal |
| Residual connections | Sapling weight (sap_w) skip connection | Normal or gated residual modes |
| **Jacobian Determinant** | | |
| Jacobian structure | Lower triangular (diagonal only matters) | Block lower triangular |
| Computation method | Numerical diff (2 forward passes) or exact jacrev | Analytical: tracked through layers via logsumexp |
| Complexity | O(N^2) per sample (diagonal extraction) | O(N^3) per sample (block matrix operations) |
| Numerical stability | delta=0.0001 approximation; clamp min=0.001 | Weight normalization + softplus final step |
| **Expressiveness** | | |
| Universal approximation | Yes (1 hidden layer sufficient) | Yes (single network) |
| Density modeling | Conditional CDF decomposition | Direct bijection modeling |
| Multi-modal support | MultiBF mixture model | Stacking multiple BNAF flows |
| **Training** | | |
| Loss function | Negative log-likelihood | Negative log-likelihood |
| Regularization | sap_w + MultiBF mixture | Weight normalization |
| Anti-memorization | MultiBF Gaussian-like constraint | Not explicitly addressed |
| Optimizer | Adam (typical) | Custom Adam/Adamax with Polyak averaging |
| Normalization | ActiNorm (data-dependent init) | Weight normalization |
| **Sampling / Inversion** | | |
| Inversion method | Batched bisection (2-stage: CDF + real space) | Sequential autoregressive inversion (analytical per dim possible, but slow) |
| Sampling source | Uniform(0.01, 0.99) | Standard Gaussian base distribution |
| Sampling speed | Iterative bisection per dimension | Sequential per dimension (analytical inverse of tanh + linear) |
| **Practical Considerations** | | |
| Code availability | This repo (PyTorch) | nicola-decao/BNAF (PyTorch) |
| Parameter count | Moderate (per-tree weights + breeze weights) | Orders of magnitude fewer than NAF |
| Scalability (dimension) | O(N^2) favorable for moderate N | O(N^3) limits high-dimensional use |
| Activation function | Sigmoid (scaled) | Tanh |
| Maturity | Research prototype | Published at UAI 2019, well-cited |

## Detailed Analysis

### Architecture Differences
BreezeForest has a distinctive modular design: each dimension gets its own "tree" (a small network), and information flows from earlier dimensions to later ones through "breeze" connections (bias terms computed from earlier dimensions' activations). This is conceptually a decomposition of the autoregressive structure into explicit per-dimension units.

BNAF, by contrast, uses a monolithic feed-forward network with carefully structured block-triangular masked weight matrices. The diagonal blocks handle within-dimension transformations (with positive weights via exp() to ensure monotonicity), while the off-diagonal blocks handle cross-dimension conditioning. This is more compact but less interpretable.

The BreezeForest "sapling weight" (sap_w) provides a learnable skip connection from input to output, similar to BNAF's optional gated residual `a*x + (1-a)*f(x)`.

### Computational Efficiency
The key advantage of BreezeForest is O(N^2) Jacobian computation vs BNAF's O(N^3). Because BreezeForest's architecture produces a strictly diagonal Jacobian for the autoregressive part (each dimension depends only on itself through the tree, with breeze providing only additive bias), the log-determinant reduces to summing log-diagonal entries. This is computed via finite differences with just 2 extra forward passes.

BNAF must track the full block lower-triangular Jacobian through the network, using logsumexp operations at each layer to accumulate the log-determinant. This involves block matrix multiplications at each layer, leading to O(N^3) scaling.

For the exact Jacobian mode, BreezeForest falls back to `torch.func.jacrev` + `slogdet`, which is O(N^3) — the same as BNAF. The finite-difference mode is the efficient default.

### Expressiveness and Universality
Both methods are universal density approximators. BreezeForest achieves this through the conditional CDF decomposition: each dimension's output is a conditional CDF (bounded in [0,1] via sigmoid), and the product of conditional CDFs can represent any joint density. BNAF achieves universality through the expressive power of its block-triangular neural network.

BreezeForest's MultiBF adds mixture modeling on top, explicitly supporting multi-modal densities. BNAF relies on stacking multiple flow layers for expressiveness, without an explicit mixture component.

### Training and Regularization
Both optimize negative log-likelihood. BreezeForest uses two regularization mechanisms: (1) the sap_w parameter that controls skip connection strength, and (2) the MultiBF mixture model that prevents sample memorization by encouraging each component to stay "Gaussian-like."

BNAF uses weight normalization for training stability and offers custom optimizers with Polyak averaging (exponential moving average of parameters), which can improve generalization.

BreezeForest's ActiNorm initialization (data-dependent bias and scale) is similar in spirit to BNAF's weight normalization — both aim to keep activations well-scaled.

### Sampling and Generation
BreezeForest inverts via batched bisection: a two-stage process that first searches in CDF space (coarse) then in real space (fine). This is general-purpose but iterative.

BNAF's inversion is also sequential per dimension (autoregressive constraint), but can analytically invert each layer's tanh + affine transform. However, for multi-layer BNAF, inversion still requires iterative methods.

BreezeForest samples from Uniform(0.01, 0.99) (matching the sigmoid output range), while BNAF uses a standard Gaussian base distribution.

## Key Insights
1. **BreezeForest's O(N^2) scaling is a genuine advantage** for moderate-to-high dimensional problems where Jacobian computation is the bottleneck.
2. **BreezeForest's modular tree structure is more interpretable** — each dimension has its own sub-network, making it easier to diagnose and debug.
3. **BNAF's analytical Jacobian tracking is more numerically principled** — no finite-difference approximation errors (delta=0.0001 in BreezeForest can introduce bias).
4. **MultiBF is a distinctive strength** — explicit mixture modeling for multi-modal densities is not standard in BNAF.
5. **BNAF's Polyak averaging** could benefit BreezeForest training stability.
6. **Both share the autoregressive sampling bottleneck** — sampling is sequential over dimensions in both methods.

## Potential Improvements for BreezeForest
- **Polyak averaging**: Adopt BNAF's exponential moving average of parameters for more stable training and potentially better generalization.
- **Weight normalization**: Consider adding weight normalization to tree weights as an alternative/complement to ActiNorm.
- **Adaptive finite-difference step**: The fixed delta=0.0001 could be replaced with an adaptive scheme that adjusts based on local curvature, reducing numerical bias.
- **Gaussian base distribution option**: Adding support for Gaussian base distributions (like BNAF) would make BreezeForest compatible with standard normalizing flow pipelines and enable comparison on standard benchmarks.
- **Gated residual connections**: BNAF's gated residual `a*x + (1-a)*f(x)` is more flexible than a simple skip connection — could enhance BreezeForest's sap_w mechanism.

## References
- [Block Neural Autoregressive Flow (arXiv:1904.04676)](https://arxiv.org/abs/1904.04676)
- [BNAF Official Implementation](https://github.com/nicola-decao/BNAF)
- [BNAF on Papers With Code](https://paperswithcode.com/paper/block-neural-autoregressive-flow)
